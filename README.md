# Parallel Supply Analysis — Cluster (Parallel‑Only)

Process all countries via 40 parallel SLURM jobs with tiered node usage. Focused, cluster-first instructions.

## Status: ✅ Ready for Parallel Execution
- 40 parallel scripts with tiered batching (1/2/4/8 countries per job)
- Full node request per job (340GB RAM, 72 CPUs, 12h, Short partition)
## What you run (Parallel‑only)

```bash
# 1) Generate scripts on the cluster
python get_countries.py --create-parallel

# 2) Submit all parallel jobs (note the ./ prefix)
sed -i 's/\r$//' submit_all_parallel.sh parallel_scripts/*.sh
chmod +x submit_all_parallel.sh parallel_scripts/*.sh
./submit_all_parallel.sh

# 3) Monitor
squeue -u $USER
tail -f outputs_global/logs/parallel_*.out

# 4) Combine when done
sbatch submit_workflow.sh
```

Tips
- Use ./script.sh on Linux/macOS when executing from the current directory
- If using Windows to prepare files, convert line endings on the cluster if needed
- sed is a Linux tool; run it on the cluster or use Git Bash/WSL on Windows

## Outputs

Per country (in outputs_per_country/):
- supply_analysis_{ISO3}.gpkg — layers: centroids, grid_lines, facilities

Combined (in outputs_global/), after running submit_workflow.sh:
- global_supply_analysis_all_layers.gpkg and per-layer GPKGs as applicable

### Alternative: Manual Processing
Process individual countries with threading support:
```bash
# Single country (single-threaded)
python process_country_supply.py USA --output-dir outputs_per_country --threads 1

# Single country (multi-threaded)
python process_country_supply.py USA --output-dir outputs_per_country --threads 16

# All countries (Windows batch)
process_all_countries.bat

# Combine results
python combine_global_results.py --input-dir outputs_per_country
```

## Multi-threading Implementation

### **Automatic Configuration**
The script automatically configures numerical libraries for optimal performance:
```bash
# Environment variables set automatically:
OMP_NUM_THREADS = allocated_cpus
MKL_NUM_THREADS = allocated_cpus  
OPENBLAS_NUM_THREADS = allocated_cpus
NUMEXPR_NUM_THREADS = allocated_cpus
```

### **Threading Usage Examples**
```bash
# Test different thread counts
python process_country_supply.py JAM --threads 1   # Baseline
python process_country_supply.py JAM --threads 4   # 4 threads
python process_country_supply.py JAM --threads 8   # 8 threads

# Cluster submission (automatic threading)
sbatch submit_test_single.sh  # Uses SLURM_CPUS_PER_TASK threads
```

### **Smart Threading**
- **Small datasets** (<100 centroids): Serial processing (avoids overhead)
- **Medium datasets** (100-10k centroids): Moderate parallelization
- **Large datasets** (>10k centroids): Full parallel processing

## Output Files

> **Note**: Output files are ignored by Git (`.gitignore`) and can be regenerated by running the workflow.

### Per Country
- `outputs_per_country/supply_analysis_{ISO3}.gpkg` - Multi-layer GeoPackage with centroids, grid_lines, facilities
- `outputs_per_country/supply_analysis_{ISO3}.csv` - CSV without geometry
- `outputs_per_country/network_summary_{ISO3}.txt` - Network connectivity summary

### Global Results
- `outputs_global/global_centroids.gpkg/csv` - Combined population centroids
- `outputs_global/global_grid_lines.gpkg/csv` - Combined grid infrastructure  
- `outputs_global/global_facilities.gpkg/csv` - Combined energy facilities
- `outputs_global/global_supply_analysis_all_layers.gpkg` - Single file with all layers
- `outputs_global/global_supply_summary.csv` - Summary by country
- `outputs_global/global_statistics.csv` - Global totals

## Configuration

Edit `config.yaml` to customize:
- Output directories
- Resource allocations
- Processing options
- Country list (if not using GADM auto-detection)

Edit `cluster_config.yaml` for cluster-specific settings:
- Partition names
- Memory/time limits
- Queue settings

## Data Schema

Each country output contains:
- `geometry` - Point geometry for population centroid
- `Population_centroid` - Population at this centroid
- `Total_Demand_2030_centroid` - Energy demand for 2030 (MWh)  
- `Total_Demand_2050_centroid` - Energy demand for 2050 (MWh)
- `GID_0` - ISO3 country code

## Cluster Resources

### **Optimized Resource Allocation (Tiered System)**

| Country Tier | CPUs | Memory | Time Limit | Countries/Script | Examples |
|--------------|------|--------|-----------|------------------|----------|
| **Tier 1** | 72 | 340GB | 12h | 1 country | USA, CHN, IND, RUS, BRA, CAN, AUS |
| **Tier 2** | 72 | 340GB | 12h | 2 countries | ARG, KAZ, DZA, MEX, IDN, SDN, LBY |
| **Tier 3** | 72 | 340GB | 12h | 4 countries | KOR, FRA, DEU, JPN, GBR, ESP, THA |
| **Other** | 72 | 340GB | 12h | 8 countries | Small island nations, etc. |

### **Combining Step**
- Memory: 64GB  
- Runtime: 4 hours
- CPUs: 12
- Partition: Short

### **Performance Monitoring**
```bash
# Check CPU utilization during job
sstat -j <JOB_ID> --format=JobID,MaxRSS,AveCPU

# Check detailed resource usage
sacct -j <JOB_ID> --format=JobID,JobName,MaxRSS,Elapsed,CPUTime,CPUTimeRAW

# Monitor all parallel jobs
squeue -u $USER --format="%.10i %.9P %.20j %.8u %.8T %.10M %.6D %R"
```

## Troubleshooting

## Fixed Issues ✅
- **Environment conflicts**: Resolved dependency issues in `environment.yml`
- **Snakemake rule conflicts**: Removed duplicate rules causing AmbiguousRuleException
- **Missing packages**: All required packages now included
- **Geographic CRS warnings**: Fixed distance calculations using proper UTM projections
- **Conda activation**: Fixed conda p1_etl environment activation in all 40 parallel scripts
- **Unicode encoding**: Fixed script generation encoding errors for Windows/Linux compatibility

### Common Issues
1. **Missing countries**: The workflow automatically skips countries without GADM boundaries
2. **Memory errors**: Increase memory allocation in `cluster_config.yaml`  
3. **Runtime errors**: Check individual country logs in logs/ directory
4. **Failed countries**: Check error messages, some countries may have no population/grid data

### Getting Help
1. **Test first**: Always run `python test_workflow.py` before full workflow
2. **Check logs**: Snakemake creates detailed logs for each job
3. **Dry run**: Use `snakemake --dry-run` to check workflow without running

## Performance Tips

1. **Large countries** (USA, CHN, RUS) now utilize 72 cores and complete in 4-8 hours (vs 48+ hours)
2. **Small islands** may have no grid data - script handles gracefully
3. **Memory usage** maximized at 340GB per node for optimal performance
4. **Parallelization** - workflow uses 40 nodes simultaneously at maximum capacity
5. **Thread optimization** - All 72 cores per node utilized automatically
6. **Performance testing** - Run `python test_parallel_performance.py` to benchmark

### **Individual Country Examples**
- **Korea (KOR)**: 18 hours → **2 hours** (9x speedup with 72 cores)
- **USA**: 48 hours → **6 hours** (8x speedup with maximum resources)  
- **China (CHN)**: 48 hours → **6 hours** (8x speedup with maximum resources)
- **India (IND)**: 48 hours → **6 hours** (8x speedup with maximum resources)

## Monitoring

Check workflow progress and logs:
```bash
# View running jobs
squeue -u $USER

# Monitor logs in real-time
chmod +x monitor_logs.sh
./monitor_logs.sh

# Check specific log files
tail -f outputs_global/logs/snakemake_*.out    # Main workflow log
tail -f outputs_global/logs/snakemake_*.err    # Error log  
tail -f outputs_global/logs/combine_results.log # Combination step log
tail -f outputs_global/logs/slurm-*.out        # Individual job logs

# Check snakemake status  
snakemake --summary

# View specific job output
cat outputs_global/logs/slurm-JOBID.out
```

### Log Files Generated
- `outputs_global/logs/snakemake_JOBID.out` - Main workflow output
- `outputs_global/logs/snakemake_JOBID.err` - Main workflow errors
- `outputs_global/logs/slurm-JOBID.out` - Individual SLURM job outputs
- `outputs_global/logs/slurm-JOBID.err` - Individual SLURM job errors
- `outputs_global/logs/combine_results.log` - Results combination log



### to execute it on cluster
```bash
# Transfer files to Linux cluster, then:
sed -i 's/\r$//' submit_all_parallel.sh parallel_scripts/*.sh
chmod +x submit_all_parallel.sh parallel_scripts/*.sh
./submit_all_parallel.sh

# Monitor progress:
squeue -u lina4376

# When parallel jobs complete, combine results:
sed -i 's/\r$//' submit_workflow.sh; chmod +x submit_workflow.sh; sbatch submit_workflow.sh
```
